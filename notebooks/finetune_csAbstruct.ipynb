{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b60f0e5-1229-4c50-a151-6988f7eb175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 11:58:24.547472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 11:58:29.705896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "from transformers import TrainingArguments\n",
    "#from main import finetune, eval, preprocess_function, calc_entropy_loss\n",
    "from sys import argv as args\n",
    "import os\n",
    "\n",
    "from datasets import DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c363ac-71a7-426b-992c-00e5b1a69880",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'CSAbstruct'\n",
    "function_names = ['eval', 'finetune', 'download', 'calc_entropy_loss']\n",
    "dataset_types = [\"train\", \"dev\", \"test\"]\n",
    "CSAbstruct_data_path = '../data/CSAbstruct/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63596025-0f04-4f2d-8865-a13988b2d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"Downloading the CSabstruct dataset from github\n",
    "    \"\"\"\n",
    "    \n",
    "    #https://github.com/allenai/sequential_sentence_classification/blob/master/data/CSAbstruct/dev.jsonl\n",
    "    CSABSTRUCT_DATA_BASE_URL = (\n",
    "        \"https://raw.githubusercontent.com/allenai/sequential_sentence_classification/master/data/CSAbstruct/\")\n",
    "\n",
    "    for dataset_type in dataset_types:\n",
    "        print(f\"Downloading {dataset_type} data...\")\n",
    "        file_name = CSAbstruct_data_path + dataset_type + '.csv'\n",
    "\n",
    "        #print(CSABSTRUCT_DATA_BASE_URL + dataset_type+ '.jsonl')\n",
    "        tempFile, headers = urlretrieve(\n",
    "            CSABSTRUCT_DATA_BASE_URL + dataset_type + '.jsonl')\n",
    "        lines = Path(tempFile).read_text(\"utf-8\").strip().splitlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            print(parse_line_json(line))\n",
    "            break\n",
    "\n",
    "        l = [parse_json(p) for p in [parse_line_json(line)\n",
    "                          for line in lines] if p is not None]\n",
    "        flat_list = []\n",
    "        for sublist in l:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        df = pd.DataFrame(flat_list)\n",
    "        \n",
    "        df.to_csv(file_name, index=False)\n",
    "        print(\"Saved at:\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92765181-db78-4847-8e24-d0adf6235b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> DatasetDict:\n",
    "    \"\"\"Loading CSAbstruct dataset from corresponding csv format\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: it contains train, validation, test datasets\n",
    "    \"\"\"\n",
    "    # dataset_dict - containing the dataset type as key and value is dataset of that type\n",
    "    dataset_dict = DatasetDict()\n",
    "    # itreate each dataset type\n",
    "    for dataset_type in dataset_types:\n",
    "        file_name = CSAbstruct_data_path + dataset_type + '.csv' # file\n",
    "        if not Path(file_name).exists():\n",
    "            print(\n",
    "                f'{dataset_type} data is not available. Tried to find at:', file_name)\n",
    "            download_data()\n",
    "\n",
    "        # load dataset\n",
    "        ds_dict = load_dataset(\"csv\", data_files = file_name)\n",
    "        \n",
    "        ds = ds_dict['train'] # train is the default value when we load the dataset from csv\n",
    "\n",
    "        # casting label column\n",
    "        ds = ds.cast_column('label', ClassLabel(names=ds.unique('label')))\n",
    "\n",
    "        # appending to dataset_dict\n",
    "        dataset_dict[dataset_type] = ds\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08146abf-4e24-4919-85bb-6dc7afe697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line_json(line: str):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json\n",
    "\n",
    "def parse_json(line:dict):\n",
    "    result_list = []\n",
    "    for i in range(len(line['sentences'])):\n",
    "        result_list.append({\"text\":line['sentences'][i],\"label\":line['labels'][i]})\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8ee538-9b5c-42e5-9383-9da14d9596ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data...\n",
      "{'abstract_id': 0, 'sentences': ['Gamification has the potential to improve the quality of learning by better engaging students with learning activities.', 'Our objective in this study is to evaluate a gamified learning activity along the dimensions of learning, engagement, and enjoyment.', 'The activity made use of a gamified multiple choice quiz implemented as a software tool and was trialled in three undergraduate IT-related courses.', 'A questionnaire survey was used to collect data to gauge levels of learning, engagement, and enjoyment.', 'Results show that there was some degree of engagement and enjoyment.', 'The majority of participants (77.63 per cent) reported that they were engaged enough to want to complete the quiz and 46.05 per cent stated they were happy while playing the quiz.', 'In terms of learning, the overall results were positive since 60.53 per cent of students stated that it enhanced their learning effectiveness.', 'A limitation of the work is that the results are self-reported and the activity was used over a short period of time.', 'Thus, future work should include longer trial periods and evaluating improvements to learning using alternative approaches to self-reported data.'], 'labels': ['background', 'objective', 'method', 'method', 'result', 'result', 'result', 'result', 'result'], 'confs': [0.7778, 0.7778, 0.7778, 1.0, 0.6111, 0.5556, 0.6111, 0.6111, 0.6111]}\n",
      "Saved at: ../data/CSAbstruct/train.csv\n",
      "Downloading dev data...\n",
      "{'abstract_id': 0, 'sentences': ['In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions.', 'The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function.', 'This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient.', 'To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy.', 'We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.'], 'labels': ['objective', 'background', 'background', 'method', 'result'], 'confs': [0.4167, 0.4167, 0.4167, 0.6389, 0.6111]}\n",
      "Saved at: ../data/CSAbstruct/dev.csv\n",
      "Downloading test data...\n",
      "{'abstract_id': 0, 'sentences': ['While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image.', 'Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results.', 'These techniques, although working well, fail to explicitly exploit the label dependencies in an image.', 'In this paper, we utilize recurrent neural networks (RNNs) to address this problem.', 'Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework.', 'Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models.'], 'labels': ['background', 'background', 'method', 'method', 'method', 'result'], 'confs': [0.7407, 0.7407, 0.7778, 0.7778, 0.7778, 1.0]}\n",
      "Saved at: ../data/CSAbstruct/test.csv\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ff66c8-1013-4188-b7ea-4a0edd1a85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-df1d9ea5bb82d2c3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a212d978b89a4365a76968595f103a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47fd39ca5ea4a628b96bfd08aef4cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69023469ed1e40e69594dfd6b3fb1a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-df1d9ea5bb82d2c3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snatesan_umass_edu/.local/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50603b4ba53d46ebac909385d8993d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434ebc1d4f0044f2a204a2408b95bfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/11333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-c309920c02222ace/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c3a8302b104558915973b60803be8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2382994ee87a4820be2e5af2cfbbf194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af34b2fd024941b088a22895a5ccf0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-c309920c02222ace/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snatesan_umass_edu/.local/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad36f19ce1f4225bfacdb97dc3c73b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871572775b3a4eeca994f91152770b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-e72d8c185594823c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a2534b6dc54d32b1961cf4b4021963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e575f7d18b24a3a8559cfb0e8633c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35be0d845adb4da9a6ce7b0980bc662c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /work/pi_adrozdov_umass_edu/snatesan_umass_edu/hf_cache/datasets/csv/default-e72d8c185594823c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snatesan_umass_edu/.local/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b203d5c75c490cb1695ee4f5823c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f50c4992ee94f4289e1b4e849d1910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23b3b7f-6588-4095-95e5-281424217c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 11333\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2026\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af2d602-32db-4635-b401-553fbf0553ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = dataset['train'], dataset['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fbf9ac-a526-4874-a955-ebed94a91b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 11333\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56072ab4-6267-41db-9ef7-30f2ced660b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data labels\n",
    "labels = train_data.features[\"label\"].names\n",
    "label2id = {labels[i]: i for i in range(len(labels))}\n",
    "id2label = {i: labels[i] for i in range(len(labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e237e696-1b4b-4693-9b0a-9eaf2e80d35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'background': 0, 'objective': 1, 'method': 2, 'result': 3, 'other': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418a227c-9bc6-4be4-956a-b49aedc63cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Based on your requirements, make changes to the variables:  checkpoints_out_dir, dataset_subset\n",
    "# model and checkpoints_out_dir directory\n",
    "L_Model = \"allenai/scibert_scivocab_uncased\"\n",
    "checkpoints_out_dir = \"../checkpoints9/csabstract\"\n",
    "# device\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00696e3-d479-4ad7-b68d-662a65adf078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "hyper_params = {\n",
    "    \"seed\" : 40,\n",
    "    \"learning_rate\" : 1e-6,\n",
    "    \"per_device_train_batch_size\" : 5,\n",
    "    \"per_device_eval_batch_size\" : 5,\n",
    "    \"num_train_epochs\" : 10, \n",
    "    \"weight_decay\" : 0.0001,\n",
    "    \"test_batch_size\": 16 }\n",
    "\n",
    "# hyper_params_X = list(hyper_params.values())\n",
    "# hyper_params_types = [int, float, int, int, int, float, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb0f3f8-3c76-432f-9cb3-a2870a5216ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random\n",
    "# torch.manual_seed(hyper_params['seed'])\n",
    "# random.seed(hyper_params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9436c38e-a3d8-440f-9dc8-35e151f74bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: \n",
    "# convert text --> ids\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38f4c9e-3567-4ac2-aa84-d9d751a0b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(L_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45d95f1b-e902-4413-8b0a-dca9b2eee92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe3654928f140c1b9275864eb496ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afccbf6b6224bdb9f4c258cc0c919c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize train and validation dataset\n",
    "train_data = train_data.map(preprocess_function, batched=True)\n",
    "valid_data = valid_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8518dd55-e70f-44fd-b3cd-f095546e6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator to form a batch from list of training dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a7ad1c-e25b-40cb-9a7e-8d49e651d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis= 1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "715de9a3-2e68-45f4-a875-4dcd5eee43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    L_Model, num_labels=len(labels), id2label=id2label, label2id=label2id, return_dict=True)\n",
    "\n",
    "# load the model into GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec552057-6a43-44b4-a0b6-26a25f3ab070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01064aa9-599a-4264-a353-140c1f016570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snatesan_umass_edu/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5660' max='5660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5660/5660 29:16, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.264000</td>\n",
       "      <td>1.486468</td>\n",
       "      <td>0.383514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.048400</td>\n",
       "      <td>1.532480</td>\n",
       "      <td>0.386476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.995700</td>\n",
       "      <td>1.613245</td>\n",
       "      <td>0.382527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.963200</td>\n",
       "      <td>1.613304</td>\n",
       "      <td>0.384995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.945800</td>\n",
       "      <td>1.641645</td>\n",
       "      <td>0.386969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>1.648427</td>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.907300</td>\n",
       "      <td>1.653294</td>\n",
       "      <td>0.388944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>1.663152</td>\n",
       "      <td>0.386476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.894700</td>\n",
       "      <td>1.664028</td>\n",
       "      <td>0.385982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>1.663424</td>\n",
       "      <td>0.386969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5660, training_loss=0.9660464768696169, metrics={'train_runtime': 1756.7615, 'train_samples_per_second': 64.511, 'train_steps_per_second': 3.222, 'total_flos': 2600202860683218.0, 'train_loss': 0.9660464768696169, 'epoch': 9.99})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoints_out_dir,\n",
    "    learning_rate=hyper_params['learning_rate'],\n",
    "    per_device_train_batch_size=hyper_params['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=hyper_params['per_device_eval_batch_size'],\n",
    "    num_train_epochs=hyper_params['num_train_epochs'],\n",
    "    gradient_accumulation_steps = 4,\n",
    "    weight_decay=hyper_params['weight_decay'],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    seed=hyper_params['seed'],\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = valid_data,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bac6a-0f16-44d3-ac36-047907a4c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate metrics\n",
    "# import random\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "\n",
    "# f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis= 1)\n",
    "#     return f1.compute(predictions=predictions, references=labels, average='micro')\n",
    "\n",
    "# # def compute_metrics(eval_pred):\n",
    "# #     predictions, labels = eval_pred\n",
    "# #     predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "# #     # calculate validation accuracy\n",
    "# #     eval_accuracy = accuracy.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "# #     # calculate train accuracy\n",
    "# #     train_predictions, train_labels = trainer.predict(train_data_s)\n",
    "# #     train_predictions = np.argmax(train_predictions, axis=1)\n",
    "# #     train_accuracy = accuracy.compute(predictions=train_predictions, references=train_labels)\n",
    "    \n",
    "# #     return {\"train_accuracy\": train_accuracy, \"eval_accuracy\": eval_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "# Trainer.hyperparameter_search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f2499-8f25-4c35-a90c-96c0bf6cc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152281ee-cef7-4969-b54d-24a9f9b94a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Union\n",
    "# from scipy import optimize\n",
    "# # Define model\n",
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# # Setting hyperparameters\n",
    "# hyper_params = {\n",
    "#     #\"seed\" : 200,\n",
    "#     \"learning_rate\" : 1e-5,\n",
    "#     \"per_device_train_batch_size\" : 5,\n",
    "#     \"per_device_eval_batch_size\" : 5,\n",
    "#     \"num_train_epochs\" : 10, \n",
    "#     \"weight_decay\" : 0.1,\n",
    "#     #\"test_batch_size\": 16,\n",
    "# }\n",
    "\n",
    "# # hyper_params_X = list(hyper_params.values())\n",
    "# # hyper_params_types = [\n",
    "# #     # int, # For seed\n",
    "# #     float, \n",
    "# #     int, \n",
    "# #     int, \n",
    "# #     int, \n",
    "# #     float, \n",
    "# #     int]\n",
    "# trainer = None\n",
    "# def loss(x: List[Union[int,float]], *args):\n",
    "#     \"\"\"\n",
    "#     Optimizer that chooses optimal hyperpermaters for a model that trains true parameters.\n",
    "#     <HACK>\n",
    "#     NOTE NOTE NOTE NOTE NOTE NOTE\n",
    "#     THE ORDER MUST ABSOLUTELY NOT CHANGE. IF YOU DO, BE SUPER DUPER CAREFUL. \n",
    "#     \"\"\"\n",
    "#     global trainer\n",
    "#     trainer = None\n",
    "#     #new_hyper_params_X = [hyper_params_types[i](i) for i in x]\n",
    "#     new_learning_rate, new_weight_decay , *none_others = x\n",
    "    \n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     L_Model, num_labels=len(labels), id2label=id2label, label2id=label2id, return_dict=True)\n",
    "\n",
    "#     # load the model into GPU\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # define training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=checkpoints_out_dir,\n",
    "#         learning_rate=new_learning_rate, #hyper_params['learning_rate'],\n",
    "#         per_device_train_batch_size=hyper_params['per_device_train_batch_size'],\n",
    "#         per_device_eval_batch_size=hyper_params['per_device_eval_batch_size'],\n",
    "#         num_train_epochs=hyper_params['num_train_epochs'],\n",
    "#         weight_decay= new_weight_decay, #hyper_params['weight_decay'],\n",
    "#         #seed=hyper_params['seed'],\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         #optim=\"adamw_torch\",\n",
    "#         #eval_steps=100,  # evaluate every 100 steps\n",
    "#         save_strategy=\"epoch\",\n",
    "#         #logging_steps=100,  # log train accuracy every 100 steps\n",
    "#     )\n",
    "\n",
    "#     # define trainer\n",
    "#     trainer = Trainer(\n",
    "#         model = model,\n",
    "#         args = training_args,\n",
    "#         train_dataset = train_data,\n",
    "#         eval_dataset = valid_data,\n",
    "#         tokenizer = tokenizer,\n",
    "#         data_collator = data_collator,\n",
    "#         compute_metrics = compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     \"\"\"\n",
    "#     self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\"\n",
    "#     {‘train_loss’: 0.7159061431884766, ‘train_accuracy’: 0.4, ‘train_f1’: 0.5714285714285715, ‘train_runtime’: 6.2973, ‘train_samples_per_second’: 2.382, ‘train_steps_per_second’: 0.159, ‘epoch’: 1.0}\n",
    "# {‘eval_loss’: 0.8529007434844971, ‘eval_accuracy’: 0.0, ‘eval_f1’: 0.0, ‘eval_runtime’: 2.0739, ‘eval_samples_per_second’: 0.964, ‘eval_steps_per_second’: 0.482, ‘epoch’: 1.0}\n",
    "    \n",
    "#     \"\"\"\n",
    "#     trainer.train()\n",
    "#     trainset_vals = trainer.evaluate(eval_dataset=train_data, metric_key_prefix='train')\n",
    "#     validation_vals = trainer.evaluate(eval_dataset=valid_data, metric_key_prefix='valid')\n",
    "#     tloss, tacc = trainset_vals['train_loss'], trainset_vals['train_accuracy']\n",
    "#     vloss, vacc = validation_vals['valid_loss'], validation_vals['valid_accuracy']\n",
    "    \n",
    "#     #loss_vac =  (0.3*tloss+0.7*vloss) + (-0.15*tacc - 0.85*vacc)\n",
    "#     loss_vac =  (-0.15*tacc - 0.85*vacc)\n",
    "#     print(\"################################\")\n",
    "#     print(f\"{new_learning_rate},{new_weight_decay}: {loss_vac:5.3f}, {tacc}, {vacc}\")\n",
    "#     return loss_vac\n",
    "    \n",
    "# best_learning_rate = optimize.minimize(loss,x0=[1e-5, 0.1],bounds=[(5e-7,5e-3), (0,0.1)],method='L-BFGS-B')\n",
    "# #best_learning_rate = optimize.minimize(loss,x0=[1e-4, 0.1],method='CG', options={'maxiter':40})\n",
    "\n",
    "# # # Train model\n",
    "# # trainer.hyperparameter_search()\n",
    "    \n",
    "# # # Train model\n",
    "# # trainer.hyperparameter_search()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0d555-bc79-45c1-9cf4-ab489b00e6f0",
   "metadata": {},
   "source": [
    "### Evaluation on test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dc80f1a-0735-4559-99ba-facfb615ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d131697f-e43e-4e9e-a938-a32647bd6d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b59cbcc-f9e4-444e-b596-617755003b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07a2eded-e6d4-4184-a10e-7747b6888134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test dataset\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# pipeline\n",
    "pipeline_task = 'text-classification'\n",
    "\n",
    "# device\n",
    "device = 'cuda:0'\n",
    "\n",
    "# model \n",
    "checkpoints_dir = '../checkpoints9/csabstract/checkpoint-5660'\n",
    "\n",
    "classifier = pipeline(pipeline_task, model=checkpoints_dir, device=device)\n",
    "\n",
    "# Make predictions on the testing dataset\n",
    "predictions = classifier(test_data['text'], batch_size=16)\n",
    "\n",
    "# Convert the predictions to a list of labels\n",
    "predicted_labels = [p['label'] for p in predictions]\n",
    "true_labels = [classifier.model.config.id2label[label] for label in test_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98ef06e4-2bef-4877-ab71-17c20ebae7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average F1 score: 0.38\n",
      "Weighted Average F1 score: 0.39\n",
      "Accuracy: 37.73%\n"
     ]
    }
   ],
   "source": [
    "# calculate f1 score for each label and accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "\n",
    "# report has three root variables 1. accuracy 2. macro avg 3. weighted avg\n",
    "macro_avg_f1_score = report['macro avg']['f1-score']\n",
    "weighted_avg_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "print('Macro Average F1 score: {:.2f}'.format(macro_avg_f1_score))\n",
    "print('Weighted Average F1 score: {:.2f}'.format(weighted_avg_f1_score))\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b85f026-657f-4c40-8b74-4e89ff98feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(report)\n",
    "\n",
    "# predictions dir\n",
    "predictions_out_dir = '../predictions/csabstruct_test.csv'\n",
    "\n",
    "df = df.transpose()\n",
    "df = df.reset_index().rename(columns={'index': 'label'})\n",
    "df = df[:-3] # removing accuracy, macro avg, weighted avg from the report\n",
    "df.insert(df.columns.get_loc('label') + 1, 'label_index', [classifier.model.config.label2id[l] for l in df['label']])\n",
    "df_sorted = df.sort_values(by='f1-score')\n",
    "df_sorted.to_csv(predictions_out_dir, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86209055-1677-466c-aeac-0d85819d1ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snatesan_umass_edu-vadops)",
   "language": "python",
   "name": "conda-env-snatesan_umass_edu-vadops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
