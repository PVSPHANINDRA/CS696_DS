{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811e9998-cb31-4e26-a754-a9ece9ec18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974085d-4c7a-490f-987d-261ea62822c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\" # Use your own API Key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070ebeb8-ab78-4820-9345-ec8087917103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-davinci-003\"\n",
    "rate_limit_per_minute = 40000 # 150000 # 1000 requests per minute\n",
    "last_rate_check_time = None\n",
    "rate_per_minute = 0\n",
    "max_total_token_cnt = 3000 # 3000 tokens per request\n",
    "def query_codex(prompt, n = 1, temperature = 0.8, max_tokens = 250, max_retries = 4, top_p = 0.75, print_prompts=False):\n",
    "    res = None\n",
    "    try_number = 0\n",
    "    global last_rate_check_time, rate_per_minute\n",
    "    max_tokens = max(max_tokens // 5, 56)\n",
    "    total_token_count = len(prompt)/4 + max_tokens # 4 tokens per char\n",
    "    if total_token_count >= max_total_token_cnt:\n",
    "        print(f\"Too big request: {total_token_count} tokens!!\")\n",
    "        yield from [] # return nothing if the request is too big\n",
    "        return\n",
    "    time_now = time.time() # time in seconds since epoch\n",
    "    temp_rate_per_minute = rate_per_minute\n",
    "    if last_rate_check_time is None:\n",
    "        last_rate_check_time = time_now\n",
    "    if time_now - last_rate_check_time > 60:\n",
    "        last_rate_check_time = time_now\n",
    "        temp_rate_per_minute = rate_per_minute * 0.98 + total_token_count\n",
    "    else:\n",
    "        temp_rate_per_minute += total_token_count\n",
    "    pred_num_req_per_minute = rate_per_minute\n",
    "    # Simple Rate limiter\n",
    "    while pred_num_req_per_minute > rate_limit_per_minute * 0.85:\n",
    "        time_now = time.time() # time in seconds since epoch\n",
    "        pred_num_req_per_minute = temp_rate_per_minute/(time_now - last_rate_check_time)\n",
    "        print(f\"Premptive sleeping to reduce rate limit: {pred_num_req_per_minute} requests per minute\")\n",
    "        time.sleep(1) # sleep for 1 second\n",
    "    if temp_rate_per_minute > rate_limit_per_minute * 0.85:\n",
    "        temp_rate_per_minute = rate_limit_per_minute * 0.42 # half the rate limit\n",
    "    if time_now - last_rate_check_time > 60:\n",
    "        last_rate_check_time = time_now\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Sending request...\")\n",
    "            print(f\"Request made: {rate_per_minute} requests per minute\")\n",
    "            print(f\"Last token count: {total_token_count}, token_request = {max_tokens}\")\n",
    "            start = time.time()\n",
    "            # print(\"=======================================================\")\n",
    "            # print(f\"{prompt}\")           \n",
    "            res = openai.Completion.create(\n",
    "                model = model_name,\n",
    "                prompt = prompt,\n",
    "                max_tokens = max_tokens,\n",
    "                temperature = temperature,\n",
    "                n = n,\n",
    "                top_p = top_p)\n",
    "            end = time.time()\n",
    "            # print(\"==========================================================\")\n",
    "            print(f\"Response received!! time taken: {end - start}\")\n",
    "            print(f\"Request made: {rate_per_minute} requests per minute\")\n",
    "            print(f\"Last token count: {total_token_count}\")\n",
    "            print(f\"Total token acc to OpenAI: {res['usage']['total_tokens']}\")\n",
    "            rate_per_minute = 0.98 * rate_per_minute + res['usage']['total_tokens']\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.APIError, openai.error.InvalidRequestError) as e:\n",
    "            if isinstance(e, openai.error.InvalidRequestError):\n",
    "                print(f\"A big request was made. Skipping... {e}\")\n",
    "                yield from []\n",
    "                return\n",
    "            else:\n",
    "                if try_number == max_retries:\n",
    "                    print(\"Reached the max rate limit error: Giving up!\")\n",
    "                    yield from []\n",
    "                    return\n",
    "                sleep_secs = 10 * (2 ** try_number) # Exponential retry\n",
    "                try_number += 1\n",
    "                time_now = time.time() # time in seconds since epoch\n",
    "                pred_num_req_per_minute = rate_per_minute/(time_now - last_rate_check_time)\n",
    "                last_rate_check_time = time.time()\n",
    "                print(e)\n",
    "                print(f\"Rate limit error #{try_number}: Sleeping for {sleep_secs} seconds...\")\n",
    "                print(f\"Current rate per minute: {pred_num_req_per_minute}, earlier rate per minute: {rate_per_minute}\")\n",
    "                time.sleep(sleep_secs)\n",
    "    for ch in res[\"choices\"]:\n",
    "        #print(\"===================================PROMPT START===================================================================================\")           \n",
    "        #print(ch[\"text\"])\n",
    "        #print(print_prompts, \"===================================PROMPT END===================================================================================\")\n",
    "        yield ch[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5fdf4fd-2faf-45c6-807f-12257986fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Ask Alexa in fun ways to quickly summarize the events on the calender today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57bc1406-e506-4c64-88f5-b11e904b2def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request...\n",
      "Request made: 353.56733881599996 requests per minute\n",
      "Last token count: 74.75, token_request = 56\n",
      "Response received!! time taken: 1.1920568943023682\n",
      "Request made: 353.56733881599996 requests per minute\n",
      "Last token count: 74.75\n",
      "Total token acc to OpenAI: 59\n",
      "\n",
      "\n",
      "\"Today you have a busy day ahead! You have a meeting at 9am, lunch at 12pm, and a conference call at 3pm. Don't forget to take a break in between to recharge!\"\n"
     ]
    }
   ],
   "source": [
    "for res in list(query_codex(prompt)):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a2ca9-34a3-4003-85e4-8640f830d028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gdhanania_umass_edu-vadops)",
   "language": "python",
   "name": "conda-env-gdhanania_umass_edu-vadops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
