import os
import json

from datetime import datetime
from datasets import Dataset, concatenate_datasets, load_dataset, ClassLabel
from transformers import TrainingArguments
from transformers import pipeline

import pandas as pd
import torch

from prompt import get_more_data 
from main import finetune, eval, calc_entropy_loss, plot as plot_data_map

from SNIPS import load_data as load_snips
from CSAbstruct import load_data as load_CSAbstruct

from sys import argv as args
from pathlib import Path

from time import time

# variables
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print("device used:", device)


def workflow(config):
    """
    Psuedo code:
        Loop(n steps):
        1. fine-tune the model
        2. eval the model
        3. generate data using LLM(chatGPT)
        4. augment the data to train data

    Args:
        config (Object): Object containing the following keys
            1. model_name_or_path - specify the model name or checkpoint path
            2. training arguments - which you need for the model
            3. dataset_name - specify the dataset name as [“clinc_oos”, “CSAbstruct”, “snips”]
            4. dataset_subset - if there any subset you wanna load. For CLINC - [“imbalanced”, “plus”, “small”]
            5. workflow_output_dir - this is where the output for each step gets saved
            6. steps - how many times the pipeline should run
            7. dynamics - specify the dataset types for which dataset cartography has to be plotted
            8. eval - specify dataset types on which evaluation should be measured(Intent class analysis and {Acc, macro_F1 score, weighted_F1 score as whole}
            9. entropy -  specify dataset types on which entropy loss for each sentence is evaluated
            10. generate_data_from - specify the dataset type to refer for data generation

    The output of the workflow is saved in the specified workflow_output_dir in the config object
        1. A folder is created with name using the time when the workflow is called and it contains subfolders with name specifying each step(0, 1, 2) and config file is copied which is used by workflow
            Each sub folder has
                1. dynamics folder- the dynamics for each dataset type saved
                2. entropy folder- contains csv file with the dataset type name. The csv file contains sentence and their entropy loss
                3. intent_analysis folder-  contains csv file with the dataset type name. The csv file contains Acc, macro_F1 score, weighted_F1 score for each intent class
                4. model folder- this is where checkpoints are saved
                5. dataMaps folder- this is where dataMaps are saved
                6. generated_data.csv - this is data generated by LLM
    """

    # logging
    print("Worflow configuration:")
    print(config)

    # variables
    dataset_types = ['train', 'validation', 'test']

    # deconstruct config object
    pretrained_model_name_or_path: str = config['model_name_or_path']
    training_args = config['training_args']
    prompt_args = config["prompts"]    
    
    ## load dataset
    dataset = None
    if config['dataset_name'] == 'clinc_oos':
        dataset = load_dataset('clinc_oos', config['dataset_subset'])
        dataset = dataset.rename_column("intent", "label")
        print('Dataset:', dataset)
        print('labels:', dataset['train'].features['label'].names)

        # filtering the oos label
        if "filter_oos_label" in config and config["filter_oos_label"]:
            print('filtering the oos label')
            dataset = dataset.filter(lambda example: example['label'] != 42) # oos label index is 42

            # updating dataset classlabel
            for __type__ in dataset:
                labels = dataset[__type__].features['label'].names
                labels.remove('oos')
                dataset[__type__] = dataset[__type__].cast_column('label', ClassLabel(names=labels))

            # updating the label index for each examples
            def update_index(example):
                if example['label'] > 42:
                    example['label'] -= 1
                return example

            dataset = dataset.map(update_index)
            print('filtered oos Dataset:', dataset)
            print('labels after oos is filtered:', dataset['train'].features['label'].names)
                
    elif config['dataset_name'] == 'snips':
        dataset = load_snips()
    elif config['dataset_name'] == 'CSAbstruct':
        dataset = load_CSAbstruct()
    
    # attr specifing whether to initiate the model from default or prev step best model at start of the step
    reinitiate_model_to_default = True
    if reinitiate_model_to_default in config:
        reinitiate_model_to_default = config['reinitiate_model_to_default']
    
    if not dataset:
        raise Exception("Datasets are improper. Please provide valid ones")
        
    train_data, eval_data, test_data = dataset['train'], dataset['validation'], dataset['test']

    output_dir: str = config['workflow_output_dir']
    steps: int = config['steps']

    workflow_folder_name = "workflow" + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    workflow_dir = os.path.join(output_dir, workflow_folder_name)

    # create workflow folder
    os.makedirs(workflow_dir)
        
    # create metrics folder
    metrics_columns = ['step', 'accuracy', 'macro_f1_score', 'weighted_f1_score']
    metrics_dir = os.path.join(workflow_dir, 'metrics')
    os.makedirs(metrics_dir)
    for _set in dataset_types:
        metrics = pd.DataFrame(columns = metrics_columns)
        metrics_file_path = os.path.join(metrics_dir, f'{_set}.csv')
        metrics.to_csv(metrics_file_path, index= False)

    # adding the workflow_config file to the current workflow directory
    workflow_config_file_path = os.path.join(workflow_dir, 'workflow_config.json')
    with open(workflow_config_file_path, 'w') as f:
        json.dump(config, f)

    # itearate
    for curr_step in range(steps):
        print('current workflow step', curr_step)
        curr_step = str(curr_step)
        
        step_start_time = time()
        # create folders for models, intent_class_analysis, dynamics, sentence_entropy, data, data_maps
        model_dir = os.path.join(workflow_dir, curr_step,  'model')
        intent_analysis_dir = os.path.join(workflow_dir, curr_step, 'intent_analysis')
        dynamics_dir = os.path.join(workflow_dir, curr_step, 'dynamics')
        entropy_dir = os.path.join(workflow_dir, curr_step, 'entropy')
        dataMaps_dir = os.path.join(workflow_dir, curr_step, 'data_maps')

        os.makedirs(model_dir)
        os.makedirs(intent_analysis_dir)
        os.makedirs(dynamics_dir)
        os.makedirs(entropy_dir)
        os.makedirs(dataMaps_dir)

        # finetune
        print('finetuning')
        finetune_start_time = time()
        training_args['output_dir'] = str(model_dir)
        model_path = finetune(pretrained_model_name_or_path, TrainingArguments(**training_args), train_data, eval_data, True, config['dynamics'], dynamics_dir)
        finetune_end_time = time()
        print(f"finetune execution time:{finetune_end_time - finetune_start_time} seconds or {(finetune_end_time - finetune_start_time) / 60} mins")

        print('plotting data Maps')
        plotting_DataMap_start_time = time()
        # plot dataMaps
        for dataset_type in config['dynamics']:
            dataset_dynamics_dir = os.path.join(dynamics_dir, dataset_type)
            title = config['dataset_name']
            if "dataset_subset" in config:
                title += f"_{config['dataset_subset']}" 
            title += f"_{dataset_type}_set"
            plot_data_map(dataset_dynamics_dir, dataMaps_dir, title)

        plotting_DataMap_end_time = time()
        print(f"plotting data map execution time:{plotting_DataMap_end_time - plotting_DataMap_start_time} seconds or {(plotting_DataMap_end_time - plotting_DataMap_start_time) / 60} mins")

        # eval
        print('run evaluation')
        eval_start_time = time()
        for _set in config['eval']:            
            intent_analysis_file_path = os.path.join(intent_analysis_dir, f'{_set}.csv')
            dataset = None
            metrics_file_path = os.path.join(metrics_dir, f'{_set}.csv')
            metrics_df = pd.read_csv(metrics_file_path)
            if _set == 'train':
                dataset = train_data
            elif _set == 'validation':
                dataset = eval_data
            elif _set == 'test':
                dataset = test_data

            acc, macro_f1, weighted_f1 =  eval(dataset, model_path, intent_analysis_file_path)
            metrics_df.loc[len(metrics_df)] = [curr_step, acc, macro_f1, weighted_f1]
            metrics_df.to_csv(metrics_file_path, index=False)
        eval_end_time = time()
        print(f"eval execution time:{eval_end_time - eval_start_time} seconds or {(eval_end_time - eval_start_time) / 60} mins")

        # calculate entropy
        print('calculate cross entropy')
        entropy_start_time = time()
        for _set in config['entropy']:
            entropy_file_path = os.path.join(entropy_dir, f'{_set}.csv')
            dataset = None
            metrics_df = None
            if _set == 'train':
                dataset = train_data
            elif _set == 'validation':
                dataset = eval_data
            elif _set == 'test':
                dataset = test_data

            calc_entropy_loss(dataset, model_path, entropy_file_path)
        
        entropy_end_time = time()
        print(f"sentence cross entropy execution time:{entropy_end_time - entropy_start_time} seconds or {(entropy_end_time - entropy_start_time) / 60} mins")

        # generate data
        print('generate data')
        data_generation_start_time = time()
        data_from = config['generate_data_from']
        prompt_llm = prompt_args["prompt_llm"]
        eg_type = prompt_args["eg_type"]
        prompt_type = int(prompt_args["prompt_type"])
        num_gen = int(prompt_args["num_gen"])
        num_eg = int(prompt_args["num_eg"])
        num_good = int(prompt_args["num_good"])
        num_bad = int(prompt_args["num_bad"])
        intent_analysis_file_path = os.path.join(intent_analysis_dir, f'{data_from}.csv')
        entropy_file_path = os.path.join(entropy_dir, f'{data_from}.csv')

        data_dict = get_more_data(prompt_type, intent_analysis_file_path, entropy_file_path,num_good,num_bad,num_eg=num_eg,num_gen=num_gen,eg_type=eg_type)

        data_generation_end_time = time()
        print(f"Data generation execution time:{data_generation_end_time - data_generation_start_time} seconds or {(data_generation_end_time - data_generation_start_time) / 60} mins")

        data_df = pd.DataFrame(columns= ['text', 'true_label'])
        for intent in data_dict:
            if data_dict[intent]: # avoiding empty arr scenario
                temp = pd.DataFrame(data_dict[intent])
                temp.columns = ['text']
                temp['true_label'] = intent
                data_df = pd.concat([data_df, temp])

        data_df.reset_index()

        # verifier
        classifier = pipeline(config['pipeline_task'], model= model_path, device=device)
        predictions = classifier(data_df['text'].tolist(), batch_size = 16)
        predicted_labels = [p['label'] for p in predictions]

        data_df['predicted_label'] = predicted_labels

        # save generated data with predicted label
        generate_data_file_path = os.path.join(workflow_dir, curr_step, 'generated_data.csv')
        data_df.to_csv(generate_data_file_path, index=False)
        print("Generated data is saved at:", Path(generate_data_file_path).absolute())


        # augment data
        print('appending generate data to train set')
        ## formatting to match the train set
        correct_df = data_df[(data_df['true_label'] == data_df['predicted_label'])]
        correct_df.drop('predicted_label', axis=1)
        correct_df = correct_df.rename(columns={'true_label': 'label'})
        correct_df['label'] = correct_df['label'].apply(lambda label : classifier.model.config.label2id[label])

        ## create dataset for df
        aug_dataset = Dataset.from_pandas(correct_df)
        aug_dataset.features["label"] = train_data.features["label"]

        train_data = concatenate_datasets([train_data, aug_dataset])
        if not reinitiate_model_to_default:
            pretrained_model_name_or_path = model_path

        step_end_time = time()
        print(f"step {curr_step} exection time:{step_end_time - step_start_time} seconds or {(step_end_time - step_start_time) / 60} mins")

if __name__ == "__main__":
    if len(args) < 2 or not os.path.exists(args[1]):
        raise Exception("Please provide config json to run config.json")
    
    config_file = args[1]
    config = None
    # Load JSON from file
    with open(config_file, 'r') as f:
      config = json.load(f)

    print('in the main')
    workflow(config)
